# Roadmap

## Projects

- Zero to Autoencoder for Classification on MNIST
- NVIDIA Parakeet Fine-tuning

## Theory

- Regularization (L1 and L2)
- Optimization Algorithms (Gradient Descent, Stochastic Gradient Descent)
- Regularization Techniques (Dropout, L1/L2 Regularization)
- Architectures:
  - Convolutional Neural Networks (CNNs)
  - Recurrent Neural Networks (RNNs)
  - Long Short-Term Memory (LSTM) Networks
  - Gated Recurrent Units (GRUs)
- Optimization Techniques
  - Advanced Optimization Algorithms (Adam, RMSprop)
  - Gradient Clipping and Exploding/Vanishing Gradients
  - Moving Averages (SMA and EMA)
- Regularization Techniques
  - DropConnect
  - Batch Normalization
  - Early Stopping
  - Cross-Validation
  - Ensemble Methods (Bagging, Boosting)
- Advanced Activation Functions (Leaky ReLU, ELU, SELU)
- Sequence Modeling
  - Sequence-to-Sequence Models
  - Attention Mechanisms
  - Transformers
- Generative Models
  - Generative Adversarial Networks (GANs)
  - Variational Autoencoders (VAEs)
- Reinforcement Learning
  - Q-Learning
    -Policy Gradients
- Hyperparameter Tuning

  - Grid Search and Random Search
  - Bayesian Optimization

- Model Interpretability and Explainability

  - SHAP (SHapley Additive exPlanations)
  - LIME (Local Interpretable Model-agnostic Explanations)

- Compression and Optimization
  - Quantization
    - Post-Training Quantization
    - Quantization-Aware Training
  - Pruning
    - Weight Pruning
    - Structured Pruning
  - Knowledge Distillation
    - Teacher-Student Networks
- Efficient Neural Network Architectures
  - MobileNets
  - EfficientNet
  - SqueezeNet
- Self-Supervised Learning
- Contrastive Learning
- Self-Supervised Pretraining
- SimCLR and MoCo

Fancy stuff

- Federated Learning
  - Privacy-Preserving Machine Learning
  - Federated Averaging (FedAvg)
- Graph Neural Networks (GNNs)
  - Graph Convolutional Networks (GCNs)
  - Graph Attention Networks (GATs)
- Capsule Networks
- Transformers Beyond NLP

- Transfomers (non-NLP)
  - Vision Transformers (ViT)
  - Multimodal Transformers
  - Applications in Image and Video Processing
- Energy-Based Models

  - Contrastive Divergence
  - Boltzmann Machines
  - Applications and Current Research

- Neural Ordinary Differential Equations (Neural ODEs)

  - Concept and Mathematical Foundation
  - Continuous Depth Models
  - Applications and Benefits

- Continual Learning
  - Lifelong Learning
  - Catastrophic Forgetting
  - Techniques for Continual Learning
- Explainable AI (XAI)

  - Advances in Model Interpretability
  - Causal Inference in Machine Learning
  - Trustworthy AI Systems

- Adversarial Machine Learning

  - Adversarial Attacks and Defenses
  - Robustness in Deep Learning Models
  - Applications in Security and Privacy

- Bio-Inspired Neural Networks

  - Spiking Neural Networks
  - Neuromorphic Computing

- Zero-Shot and Few-Shot Learning
- Advanced Reinforcement Learning Techniques
  - Deep Q-Networks (DQN)
  - Proximal Policy Optimization (PPO)
  - Applications in Robotics and Game Playing
- Generative Models
  - Advanced Generative Adversarial Networks (GANs)
  - StyleGAN
  - BigGAN
  - Flow-Based Models
